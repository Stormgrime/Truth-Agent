# Neo4j Connection Details
NEO4J_URI="bolt://localhost:7687"
NEO4J_USER="neo4j"
NEO4J_PASSWORD="password"

# Hugging Face Settings
HF_HOME="./.cache/huggingface" # Optional: Specify a local cache directory for Hugging Face models

# LLM Configuration
LLM_MODEL_NAME="Qwen/Qwen1.5-1.8B-Chat" # Default LLM model to use (e.g., from Hugging Face Hub)
# Or for a local vLLM server:
# LLM_API_BASE_URL="http://localhost:8000/v1" # If using vLLM OpenAI-compatible server
# LLM_API_KEY="YOUR_VLLM_API_KEY_IF_ANY" # Usually not needed for local vLLM

# Embedding Model Configuration
EMBEDDING_MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2" # Default embedding model

# Application Settings
LOG_LEVEL="INFO" # Logging level (e.g., DEBUG, INFO, WARNING, ERROR)

# Retrieval Settings (examples, can be expanded in config.py)
CONTEXT_WINDOW_SIZE=8000 # Max tokens in LLM context (example, Qwen1.5-1.8B has 32k)
COMPACTION_TRIGGER_TOKENS=6000 # Trigger compaction when prompt tokens exceed this
COMPACTION_OLDEST_TOKENS=2000 # Number of oldest tokens to compact

# Metacognition Settings (examples)
REFLECTION_INTERACTION_INTERVAL=5 # Trigger reflection every N interactions
