name: Python CI

on: [push, pull_request]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10"]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: "1.7.1" 
        virtualenvs-create: true
        virtualenvs-in-project: true

    - name: Get HF_HOME path
      id: hf_home_path
      run: echo "hf_home=$(python -c 'from pathlib import Path; print(Path.home() / ".cache" / "huggingface")')" >> $GITHUB_OUTPUT

    - name: Load cached HuggingFace hub directory
      id: cached-hf-hub
      uses: actions/cache@v4
      with:
        path: ${{ steps.hf_home_path.outputs.hf_home }}/hub
        key: hf-hub-${{ runner.os }}-${{ hashFiles('**/pyproject.toml') }} # Key on pyproject.toml to bust cache if models change potentially

    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v4
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('**/poetry.lock') }}
        restore-keys: |
          venv-${{ runner.os }}-${{ matrix.python-version }}-

    - name: Install dependencies
      run: poetry install --no-interaction # Installs dev dependencies too by default

    - name: Run Ruff Linter
      run: poetry run ruff check graph_llm_agent tests

    - name: Run Black Formatter Check
      run: poetry run black --check graph_llm_agent tests

    - name: Run Pytest tests
      run: |
        poetry run pytest -q tests/

    - name: Run Smoke Test
      env:
        PYTHONPATH: . 
        # Override model names for CI to use tiny, fast-loading models
        LLM_MODEL_NAME: "hf-internal-testing/tiny-random-gpt2"
        EMBEDDING_MODEL_NAME: "hf-internal-testing/tiny-random-bert" 
        # Ensure these models are compatible with the clients or that clients mock them effectively in a CI mode
      run: |
        poetry run python -m graph_llm_agent.main smoke --model ${{ env.LLM_MODEL_NAME }}
        # The --model flag in main.py's smoke command will use this env var if not overridden by an explicit value there.
        # It's better to pass the model directly to the command for clarity.
        # poetry run python -m graph_llm_agent.main smoke --model "hf-internal-testing/tiny-random-gpt2"
        # The current main.py smoke command default for --model is settings.LLM_MODEL_NAME.
        # The env var LLM_MODEL_NAME will be picked up by settings, so this should work.
```
